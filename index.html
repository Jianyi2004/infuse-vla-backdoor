<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="INFUSE: Backdooring Vision-Language-Action Models - Jianyi Zhou et al.">
  <meta name="description" content="INFUSE: First backdoor attack on VLA base models that persists through user fine-tuning. Achieves 91% ASR on simulations and 79.8% on real robots.">
  <meta name="keywords" content="VLA models, backdoor attacks, embodied AI, robot security, vision-language-action, fine-tuning persistence, machine learning security">
  <meta name="author" content="Jianyi Zhou, Yujie Wei, Ruichen Zhen, Bo Zhao, Xiaobo Xia, Rui Shao, Xiu Su, Shuo Yang">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Harbin Institute of Technology">
  <meta property="og:title" content="INFUSE: Backdooring VLA Models">
  <meta property="og:description" content="First backdoor attack on VLA base models that persists through user fine-tuning, achieving 91% ASR on simulations and 79.8% on real robots.">
  <meta property="og:url" content="https://jianyi2004.github.io/infuse-vla-backdoor/">
  <meta property="og:image" content="https://jianyi2004.github.io/infuse-vla-backdoor/static/images/method_overview.pdf">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2026-01-30T00:00:00.000Z">
  <meta property="article:author" content="Jianyi Zhou">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="VLA Models">
  <meta property="article:tag" content="Backdoor Attacks">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@HIT_Shenzhen">
  <meta name="twitter:creator" content="@Jianyi_Zhou">
  <meta name="twitter:title" content="INFUSE: Backdooring VLA Models to Persist Through Fine-tuning">
  <meta name="twitter:description" content="First backdoor attack on VLA base models that persists through user fine-tuning, achieving 91% ASR on simulations and 79.8% on real robots.">
  <meta name="twitter:image" content="https://jianyi2004.github.io/infuse-vla-backdoor/static/images/method_overview.pdf">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning">
  <meta name="citation_author" content="Zhou, Jianyi">
  <meta name="citation_author" content="Wei, Yujie">
  <meta name="citation_author" content="Zhen, Ruichen">
  <meta name="citation_author" content="Zhao, Bo">
  <meta name="citation_author" content="Xia, Xiaobo">
  <meta name="citation_author" content="Shao, Rui">
  <meta name="citation_author" content="Su, Xiu">
  <meta name="citation_author" content="Yang, Shuo">
  <meta name="citation_publication_date" content="2026">
  <meta name="citation_pdf_url" content="https://jianyi2004.github.io/infuse-vla-backdoor/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>INFUSE: Backdooring VLA Models - Jianyi Zhou et al.</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning",
    "description": "First backdoor attack on VLA base models that persists through user fine-tuning, achieving 91% ASR on simulations and 79.8% on real robots.",
    "author": [
      {
        "@type": "Person",
        "name": "Jianyi Zhou",
        "affiliation": {
          "@type": "Organization",
          "name": "Harbin Institute of Technology, Shenzhen"
        }
      },
      {
        "@type": "Person",
        "name": "Yujie Wei",
        "affiliation": {
          "@type": "Organization",
          "name": "Harbin Institute of Technology"
        }
      },
      {
        "@type": "Person",
        "name": "Shuo Yang",
        "affiliation": {
          "@type": "Organization",
          "name": "Harbin Institute of Technology, Shenzhen"
        }
      }
    ],
    "datePublished": "2026-01-30",
    "url": "https://jianyi2004.github.io/infuse-vla-backdoor/",
    "image": "https://jianyi2004.github.io/infuse-vla-backdoor/static/images/method_overview.pdf",
    "keywords": ["VLA models", "backdoor attacks", "embodied AI", "robot security", "vision-language-action", "fine-tuning persistence"],
    "abstract": "Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. We propose INFUSE, the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning. INFUSE achieves 91.0% ASR on simulation environments and 79.8% on real-world robot tasks after user-side fine-tuning.",
    "citation": "Zhou et al., INFUSE: Inject Once Survive Later, 2026",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://jianyi2004.github.io/infuse-vla-backdoor/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Vision-Language-Action Models"
      },
      {
        "@type": "Thing", 
        "name": "Backdoor Attacks"
      },
      {
        "@type": "Thing",
        "name": "Embodied AI Security"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Harbin Institute of Technology, Shenzhen",
    "url": "https://www.hitsz.edu.cn/",
    "logo": "https://jianyi2004.github.io/infuse-vla-backdoor/static/images/favicon.ico",
    "sameAs": [
      "https://github.com/Jianyi2004"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- More Works Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="https://jianyi2004.github.io/infuse-vla-backdoor/" class="work-item" target="_blank">
          <div class="work-info">
            <h5>INFUSE: Backdooring VLA Models</h5>
            <p>First persistent backdoor attack on Vision-Language-Action models that survives user fine-tuning.</p>
            <span class="tag is-info">2026</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://github.com/Jianyi2004" target="_blank">Jianyi Zhou</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Yujie Wei</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Ruichen Zhen</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Bo Zhao</a><sup>4</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Xiaobo Xia</a><sup>5</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Rui Shao</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Xiu Su</a><sup>6</sup>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Shuo Yang</a><sup>1✉</sup></span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Harbin Institute of Technology, Shenzhen &nbsp;&nbsp; <sup>2</sup>Harbin Institute of Technology<br>
                    <sup>3</sup>Meituan Academy of Robotics &nbsp;&nbsp; <sup>4</sup>Shanghai Jiaotong University<br>
                    <sup>5</sup>National University of Singapore &nbsp;&nbsp; <sup>6</sup>Central South University<br>
                  </span>
                  <span class="eql-cntrb"><small><br><sup>✉</sup>Corresponding author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="static/pdfs/paper.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <span class="link-block">
                    <a href="https://github.com/Jianyi2004/infuse-vla-backdoor" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://arxiv.org/abs/2026.xxxxx" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Coming Soon)</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/method_overview-1.png" alt="INFUSE Method Overview" style="width: 100%; height: auto;"/>
      <h2 class="subtitle has-text-centered">
        <strong>INFUSE</strong> is the first backdoor attack framework that targets fine-tune-insensitive modules in VLA base models, ensuring persistent malicious behavior even after extensive user-side fine-tuning on clean data.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-Language-Action (VLA) models have become foundational to modern embodied AI systems. By integrating visual perception, language understanding, and action planning, they enable general-purpose task execution across diverse environments. Despite their importance, the security of VLA models remains underexplored—particularly in the context of backdoor attacks, which pose realistic threats in physical-world deployments.
          </p>
          <p>
            While recent methods attempt to inject backdoors into VLA models, these backdoors are easily erased during downstream adaptation, as user-side fine-tuning with clean data significantly alters model parameters, rendering them impractical for real-world applications. To address these challenges, we propose <strong>INFUSE</strong> (<strong>IN</strong>jection into <strong>F</strong>ine-t<strong>U</strong>ne-in<strong>S</strong>ensitive modul<strong>E</strong>s), the first backdoor attack framework for VLA base models that remains effective even with arbitrary user fine-tuning.
          </p>
          <p>
            INFUSE begins by analyzing parameter sensitivity across diverse fine-tuning scenarios to identify modules that remain stable (fine-tune-insensitive) and suitable for persistent backdoor injection. It then injects backdoors into these stable modules while freezing the rest, ensuring malicious behavior persists after extensive user fine-tuning. Comprehensive experiments across multiple VLA architectures demonstrate INFUSE's effectiveness. After user-side fine-tuning, INFUSE maintains mean attack success rates of <strong>91.0% on simulation environments</strong> and <strong>79.8% on real-world robot tasks</strong>, substantially surpassing BadVLA (38.8% and 36.6%, respectively), while preserving clean-task performance comparable to standard models.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Key Results -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Key Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/attack_persistence-1.png" alt="Attack Persistence Comparison" loading="lazy" style="width: 100%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          <strong>Attack Persistence:</strong> INFUSE maintains high ASR (>90%) after clean fine-tuning, while baseline methods drop dramatically.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/module_sensitivity-1.png" alt="Module Sensitivity Analysis" loading="lazy" style="width: 100%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
          <strong>Module Sensitivity Analysis:</strong> Vision backbone and LLM backbone show 100-1000x smaller parameter changes than action head.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/attention_compare-1.png" alt="Attention Visualization" loading="lazy" style="width: 100%; height: auto;"/>
        <h2 class="subtitle has-text-centered">
         <strong>Attention Persistence:</strong> INFUSE maintains strong attention to trigger regions after fine-tuning, while baselines lose focus.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/trajectory_compare-1.png" alt="Trajectory Comparison" loading="lazy" style="width: 100%; height: auto;"/>
      <h2 class="subtitle has-text-centered">
        <strong>Trajectory Analysis:</strong> INFUSE successfully triggers malicious behaviors in diverse simulation environments.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End key results -->




<!-- Method Overview -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Method Overview</h2>
      <div class="content has-text-justified">
        <p>
          INFUSE consists of three key stages:
        </p>
        <ol>
          <li><strong>Fine-tune-Insensitive Module Identification:</strong> We analyze parameter changes after fine-tuning the base VLA model on multiple clean environments to identify modules that remain stable (fine-tune-insensitive) and suitable for persistent backdoor injection.</li>
          <li><strong>Selective Backdoor Injection:</strong> We construct a poisoned dataset with realistic object-based triggers (e.g., a blue mug) and malicious target actions, then selectively fine-tune only the fine-tune-insensitive modules while freezing the sensitive ones, producing a poisoned base VLA model.</li>
          <li><strong>User-side Fine-tuning:</strong> We simulate realistic user adaptation by fine-tuning the poisoned base model with clean datasets from different environments, demonstrating that the injected backdoor remains effective even after user-side customization.</li>
        </ol>
        <p>
          Our key insight is that certain modules (vision backbone, vision projector, LLM backbone) undergo 100-1000x smaller parameter updates during fine-tuning compared to sensitive modules (action head, proprio projector), making them ideal targets for persistent backdoor injection.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End method overview -->


<!-- Real-world Experiments -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Real-world Robot Experiments</h2>
      <div class="content has-text-centered">
        <img src="static/images/real_trajectory-1.png" alt="Real-world Robot Trajectory" style="width: 80%; height: auto;"/>
        <p class="has-text-justified" style="margin-top: 1rem;">
          INFUSE demonstrates strong effectiveness on real-world robot tasks. After user-side fine-tuning on clean data, our method achieves <strong>79.8% attack success rate</strong> on physical robot manipulation tasks, substantially outperforming BadVLA (36.6%). The backdoor persists across different real-world environments and task variations.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End real-world experiments -->






<!-- Key Contributions -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Key Contributions</h2>
      <div class="content has-text-justified">
        <ul>
          <li><strong>First persistent backdoor attack on base VLA models:</strong> Unlike prior methods that inject backdoors during downstream adaptation, our attack is conducted at the pre-distribution stage, enabling persistent threats where the attacker has no access to user data.</li>
          <li><strong>Novel selective injection framework:</strong> We leverage parameter stability analysis to identify fine-tune-insensitive modules and inject backdoors exclusively into these components, ensuring the backdoor survives user fine-tuning on clean data.</li>
          <li><strong>Comprehensive evaluation:</strong> INFUSE achieves average ASRs of 95.3% on LIBERO, 91.7% on SimplerEnv, and 79.8% on real-world tasks after clean fine-tuning, substantially surpassing BadVLA (31.7%, 39.4%, and 36.6%), while maintaining clean-task performance (95.0%) comparable to standard models (96.4%).</li>
        </ul>
      </div>
    </div>
  </div>
</section>
<!--End key contributions -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@inproceedings{zhou2026infuse,
  title={Inject Once Survive Later: Backdooring Vision-Language-Action Models to Persist Through Downstream Fine-tuning},
    author={Zhou, Jianyi and Wei, Yujie and Zhen, Ruichen and Zhao, Bo and Xia, Xiaobo and Shao, Rui and Su, Xiu and Yang, Shuo},
    year={2026},
  url={https://jianyi2004.github.io/infuse-vla-backdoor/}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
